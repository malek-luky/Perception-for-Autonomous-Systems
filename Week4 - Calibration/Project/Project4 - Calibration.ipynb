{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT WEEK 4\n",
    "* This week you'll be provided with a set of stereo images, and your task is to undistort and rectify the images, such that they can be used with the stereo depth reconstruction you made on day 6. You are still not allowed to use opencv functions for block matching/template matching, however you are free to use opencv when undistorting the images. It is recommended that you finish the exercises from Monday before continuing with the weekly project.\n",
    "* The image sets are found in the attached zip-file. Start with the rs.zip and move on to mynteye.zip once you have it working. The physical dimensions of each square of the pattern are 33.6 x 33.6 mm.\n",
    "* Hint: You'll have to undistort the images before rectifying them.\n",
    "* Hint: Some of the relevant functions you'll be using can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### So far the best procedure found has been:\n",
    "\n",
    "#### Undistortion images\n",
    "1. For all right and left images find the chessboardcorners: objectpoints IRL and imagepoint on imageplane.\n",
    "2. Use cv2.CalibrateCamera() to get distortion coefficients from the imagepoints\n",
    "3. Then use cv2.getOptimalNewCameraMatrix(), which is RANSAC + SVD (DLT) to get optimal K matrix.\n",
    "\n",
    "#### Stereo calibration\n",
    "1. use cv2.stereoCalibrate() to get R, t, E and F matrix connecting the cameras..\n",
    "2. Then, use cv2.stereoRectify() to get rotation, translation and projection of each camera.\n",
    "3. Finally we can use cv2.initUndistortRectifyMap() to create a complete map, which can be used to undistort and rectify all images from the camera. \n",
    "        - This is applied using \"cv2.remap()\"\n",
    "\n",
    "#### Alternatively\n",
    "\n",
    "A more intuitive approach, closer to the math, not using stereoCalibrate() can be found from ex4.\n",
    "Again use cv2.getOptimalNewCameraMatrix() on all corners of the chessboard to estimate the camera matrices K1, K2.\n",
    "\n",
    "Distortion coefficients for both cameras are again found using the imagepoints and cv2.CalibrateCamera()\n",
    "\n",
    "THen find F using cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_RANSAC) where pt1 and pts2 correspond to the 200 best matches found between the two images. Since some of them are outliers its nice to use RANSAC.\n",
    "\n",
    "E matrix is then found using the definition of E and F: E = K_left.T@F@K_right. \n",
    "\n",
    "The rotations and translation is given by cv2.decomposeEssentialMat(E): NB! its just two possible rotations being returned, not left and right!\n",
    "\n",
    "Then instead of using cv2.stereoRectify() we use the definition of the projection matrices to get them:\n",
    "P_left = np.hstack((K_left@R_left, K_left@t))\n",
    "P_right = np.hstack((K_right@R_right, K_right@t))\n",
    "\n",
    "\n",
    "Now we are ready to produce some maps! cv2.initUndistortRectifyMap(K, dist, R, P,(w,h),cv2.CV_32FC1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDISTORTION\n",
    "* Find the chessboard corners and undistort the image\n",
    "* Camera matrix calculated separately for left and right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mtx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lukas\\OneDrive - České vysoké učení technické v Praze\\Zaloha\\Lukas\\DTU\\Perception for Autonomous Systems\\Exercises\\Week4\\Project\\Project4.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukas/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/Zaloha/Lukas/DTU/Perception%20for%20Autonomous%20Systems/Exercises/Week4/Project/Project4.ipynb#ch0000004?line=33'>34</a>\u001b[0m img_left \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mimgs/left-0000.png\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# just to get dimensions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukas/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/Zaloha/Lukas/DTU/Perception%20for%20Autonomous%20Systems/Exercises/Week4/Project/Project4.ipynb#ch0000004?line=34'>35</a>\u001b[0m h,  w \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lukas/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/Zaloha/Lukas/DTU/Perception%20for%20Autonomous%20Systems/Exercises/Week4/Project/Project4.ipynb#ch0000004?line=35'>36</a>\u001b[0m K_left, roi \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mgetOptimalNewCameraMatrix(mtx,dist,(w,h),\u001b[39m1\u001b[39m,(w,h))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukas/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/Zaloha/Lukas/DTU/Perception%20for%20Autonomous%20Systems/Exercises/Week4/Project/Project4.ipynb#ch0000004?line=37'>38</a>\u001b[0m \u001b[39m# save images into folder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukas/OneDrive%20-%20%C4%8Cesk%C3%A9%20vysok%C3%A9%20u%C4%8Den%C3%AD%20technick%C3%A9%20v%20Praze/Zaloha/Lukas/DTU/Perception%20for%20Autonomous%20Systems/Exercises/Week4/Project/Project4.ipynb#ch0000004?line=38'>39</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mtx' is not defined"
     ]
    }
   ],
   "source": [
    "rerun_camera_matrix = True\n",
    "\n",
    "##################### LEFT IMAGES #########################\n",
    "if rerun_camera_matrix == True:   \n",
    "    # Implement the number of vertical and horizontal corners\n",
    "    nb_vertical = 9\n",
    "    nb_horizontal = 6\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((nb_horizontal*nb_vertical,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nb_vertical,0:nb_horizontal].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d point in real world space\n",
    "    imgpoints_left = [] # 2d points in image plane.\n",
    "\n",
    "    images = glob.glob('imgs/left*.png')\n",
    "    assert images\n",
    "\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Implement findChessboardCorners here\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nb_vertical, nb_horizontal))\n",
    "\n",
    "        # If found, add object points, image points (after refining them)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints_left.append(corners)\n",
    "\n",
    "    # get the camera matrix\n",
    "    ret, mtx_left, dist_left, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints_left, gray.shape[::-1], None, None)\n",
    "    img_left = cv2.imread('imgs/left-0000.png') # just to get dimensions\n",
    "    h,  w = img.shape[:2]\n",
    "    K_left, roi = cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "\n",
    "    # save images into folder\n",
    "    i = 0\n",
    "    for fname in images:\n",
    "        # undistort\n",
    "        img = cv2.imread(fname)\n",
    "        dst = cv2.undistort(img_left, mtx_left, dist_left, None, K_left)\n",
    "\n",
    "        # crop the image\n",
    "        x,y,w,h = roi\n",
    "        dst = dst[y:y+h, x:x+w]\n",
    "\n",
    "        # save image\n",
    "        cv2.imwrite('undistorted/left'+str(i)+'.png',dst)\n",
    "        i+=1\n",
    "\n",
    "    ##################### RIGHT IMAGES #########################\n",
    "\n",
    "    # Implement the number of vertical and horizontal corners\n",
    "    nb_vertical = 9\n",
    "    nb_horizontal = 6\n",
    "\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((nb_horizontal*nb_vertical,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nb_vertical,0:nb_horizontal].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d point in real world space\n",
    "    imgpoints_right = [] # 2d points in image plane.\n",
    "\n",
    "    images = glob.glob('imgs/right*.png')\n",
    "    assert images\n",
    "\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Implement findChessboardCorners here\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nb_vertical, nb_horizontal))\n",
    "\n",
    "        # If found, add object points, image points (after refining them)\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints_right.append(corners)\n",
    "\n",
    "    # get the camera matrix\n",
    "    ret, mtx_right, dist_right, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "    img_right = cv2.imread('imgs/right-0000.png') # just to get dimensions\n",
    "    h,  w = img.shape[:2]\n",
    "    K_right, roi = cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))\n",
    "\n",
    "    # save images into folder\n",
    "    i = 0\n",
    "    for fname in images:\n",
    "        # undistort\n",
    "        img = cv2.imread(fname)\n",
    "        dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n",
    "\n",
    "        # crop the image\n",
    "        x,y,w,h = roi\n",
    "        dst = dst[y:y+h, x:x+w]\n",
    "\n",
    "        # save image\n",
    "        cv2.imwrite('undistorted/right'+str(i)+'.png',dst)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECTIFY THE IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Find the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load undistorted images\n",
    "img_left = cv2.imread('imgs/left-0013.png',0)\n",
    "img_right = cv2.imread('imgs/right-0013.png',0)\n",
    "\n",
    "# Create a sift detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp_left, des_left = sift.detectAndCompute(img_left, None)\n",
    "kp_right, des_right = sift.detectAndCompute(img_right, None)\n",
    "kp_img_left = cv2.drawKeypoints(img_left, kp_left, img_left, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp_img_right = cv2.drawKeypoints(img_right, kp_right, img_right, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(15,15))\n",
    "ax1.imshow(kp_img_left)\n",
    "ax2.imshow(kp_img_right)\n",
    "plt.figure(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Match the points\n",
    "- we have 200 points with some outliers\n",
    "- SVD would explode for 200 points with so many outliers\n",
    "- RANSAC is choosing 8 random points for SVD (subgroup, we need 4 pairs of points to get the F matrix)\n",
    "- F matrix maps one points to a line\n",
    "- Homography works only for planes (like a points on chessboard where we can reduce one dimension)\n",
    "- transpose F if we want to get the points the other way (img2->img1)\n",
    "- The essential matrix includes the pose of the cameras with respect to each other\n",
    "- The fundamental matrix projects a point in the right image frame to a point in the left image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = cv2.BFMatcher().match(des_left, des_right)\n",
    "\n",
    "# Sort them in the order of their distance (i.e. best matches first).\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "nb_matches = 200\n",
    "\n",
    "good = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "# Using 200 best matches\n",
    "for m in matches[:nb_matches]:\n",
    "    good.append(m)\n",
    "    # Extract points corresponding to matches.\n",
    "    pts1.append(kp_left[m.queryIdx].pt)\n",
    "    pts2.append(kp_right[m.trainIdx].pt)\n",
    "\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Get the fundamental matrix\n",
    "* As stated before, we use RAMSAC combined with SVD to find the fundamental matrix\n",
    "* Simply we just pair 8 points until we find the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fundamental matrix\n",
    "F, mask =cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_RANSAC)\n",
    "print(F)\n",
    "\n",
    "# remove outliers\n",
    "pts1 = pts1[mask.ravel() == 1]\n",
    "pts2 = pts2[mask.ravel() == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Draw the epipolar lines\n",
    "- if we pass the image into function, it somehow rewrites also the arguments, its a mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img1,img2,lines,pts1,pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    r,c = img1.shape\n",
    "    img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)\n",
    "    img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)\n",
    "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,2)\n",
    "        img1 = cv2.circle(img1,tuple(pt1),5,color,-1)\n",
    "        img2 = cv2.circle(img2,tuple(pt2),5,color,-1)\n",
    "    return img1,img2\n",
    "\n",
    "# Find epilines corresponding to points in right image (second image) and\n",
    "# drawing its lines on left image\n",
    "lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2 ,F)\n",
    "lines1 = lines1.reshape(-1, 3)\n",
    "epilines_left, keypoints_left = draw_lines(img_left, img_right, lines1, pts1, pts2)\n",
    "\n",
    "# Find epilines corresponding to points in left image (first image) and\n",
    "# drawing its lines on right image\n",
    "lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)\n",
    "lines2 = lines2.reshape(-1, 3)\n",
    "epilines_right, keypoints_right = draw_lines(img_right, img_left, lines2, pts2, pts1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, constrained_layout=True, figsize=(10,10))\n",
    "axs[0, 0].imshow(keypoints_right)\n",
    "axs[0, 0].set_title('left keypoints')\n",
    "axs[0, 1].imshow(keypoints_left)\n",
    "axs[0, 1].set_title('right keypoints')\n",
    "axs[1, 0].imshow(epilines_left)\n",
    "axs[1, 0].set_title('left epipolar lines')\n",
    "axs[1, 1].imshow(epilines_right)\n",
    "axs[1, 1].set_title('right epipolar lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Find the Projection matrices\n",
    "* Can be down boring way using the following calibration function\n",
    "* cv2.stereoCalibrate()\n",
    "* cv2.stereoRectify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = K_left.T@F@K_right\n",
    "R_left, R_right, t = cv2.decomposeEssentialMat(E)\n",
    "cv2.stereoRectify(K_left, dist_left, K_right, dist_right, img.shape[:2], R_left, t ) #Try R1 change to R_left, if not working, mistake is here\n",
    "P_left = np.hstack((K_left@R_left, K_left@t))\n",
    "P_right = np.hstack((K_right@R_right, K_right@t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Rectify the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load undistorted images (without beeing messed up for some unknown reason)\n",
    "leftMapX, leftMapY = cv2.initUndistortRectifyMap(K_left,dist_left,R_left,P_left,(w,h),cv2.CV_32FC1)\n",
    "left_rectified = cv2.remap(img_left,leftMapX,leftMapY,cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "\n",
    "rightMapX, rightMapY = cv2.initUndistortRectifyMap(K_right,dist_right,R_right,P_right,(w,h),cv2.CV_32FC1)\n",
    "right_rectified = cv2.remap(img_right,rightMapX,rightMapY,cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "\n",
    "#TODO: crop the black parts of the image\n",
    "plt.imshow(left_rectified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Visualize the changes\n",
    "* following exercise 2 - epipolar lines (deleted all the comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = left_rectified.copy()\n",
    "img2 = right_rectified.copy()\n",
    "\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "kp_img_left = cv2.drawKeypoints(img1, kp1, img1, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp_img_right = cv2.drawKeypoints(img2, kp2, img2, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "\n",
    "matches = cv2.BFMatcher().match(des1, des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "nb_matches = 200\n",
    "good = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for m in matches[:nb_matches]:\n",
    "    good.append(m)\n",
    "    pts1.append(kp1[m.queryIdx].pt)\n",
    "    pts2.append(kp2[m.trainIdx].pt)\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "    \n",
    "F, mask =cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_RANSAC)\n",
    "pts1 = pts1[mask.ravel() == 1]\n",
    "pts2 = pts2[mask.ravel() == 1]\n",
    "\n",
    "def draw_lines(img1,img2,lines,pts1,pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    r,c = img1.shape\n",
    "    img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)\n",
    "    img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)\n",
    "    for r,pt1,pt2 in zip(lines,pts1,pts2):\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color,2)\n",
    "        img1 = cv2.circle(img1,tuple(pt1),5,color,-1)\n",
    "        img2 = cv2.circle(img2,tuple(pt2),5,color,-1)\n",
    "    return img1,img2\n",
    "\n",
    "lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2 ,F)\n",
    "lines1 = lines1.reshape(-1, 3)\n",
    "img5, img6 = draw_lines(img1, img2, lines1, pts1, pts2)\n",
    "lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)\n",
    "lines2 = lines2.reshape(-1, 3)\n",
    "img3, img4 = draw_lines(img2, img1, lines2, pts2, pts1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, constrained_layout=True, figsize=(10,10))\n",
    "axs[0, 0].imshow(img4)\n",
    "axs[0, 0].set_title('left keypoints')\n",
    "axs[0, 1].imshow(img6)\n",
    "axs[0, 1].set_title('right keypoints')\n",
    "axs[1, 0].imshow(img5)\n",
    "axs[1, 0].set_title('left epipolar lines')\n",
    "axs[1, 1].imshow(img3)\n",
    "axs[1, 1].set_title('right epipolar lines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69eb92836b941e979072a76c7fcfffe5419cca933cedd02cfafbdfca1a93358c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
